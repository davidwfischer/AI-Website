<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Functionality</title>
    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="Style.css">
</head>

<body>
    <nav>
        <a href="Geschichte.html">←</a>
        <a href="index.html">Start</a>
        <a href="Arten.html">→ </a>
    </nav>
    <h1>Funktionsweisen</h1>
    <h2>How does Large Language Models works?</h2>
    <p>
    <ol>
        <p>
            <li>Architektur: Transformer-Network</li>
        <ul>
            <li> Most Large Language Models use the Transformer architecture.which is designed for sequence-to-sequence tasks such as machine translation. With encoder and decoder stacks, residual connections and positional encoding, the Transformer enables complex processing of sequences and has achieved impressive results in natural language processing.</li>
        </ul>
        <img id="Transformer" src="TRANSFORMER.png" alt="">

        </p>

        <p>
            <li>
                Embeddings: Encoding Words and Context</li>
        <ul>
            <li>Words are converted into vectors called embeddings. These vectors capture the correct
                Meaning of words and their context.</li>
        </ul>
        </p>
        <p>
            <li>
                Self-attention mechanism: processing context</li>
        <ul>
            <li>The cornel of the Transformer model is the self-attention mechanism. This makes it possible
                Model to focus on different parts of the input sequence and better understand the context
                </li>
        </ul>
        </p>
        <p>
            <li> Pre-training on large data sets </li>
        <ul>
            <li>The model is first pre-trained on a huge data set consisting of texts.The model is first pre-trained on a huge data set consisting of texts. It learns to understand patterns and structures in the language without being specifically trained for a particular task.</li>
        </ul>
        </p>
        <p>
            <li>
                Adaptation to specific tasks:</li>
        <ul>
            <li> After training, the model is fine-tuned for specific tasks. This can include adapting to specific text types, translations, or other tasks.

            </li>
        </ul>
        </p>
        <p>
            <li>
                Areas of application: Diverse tasks</li>
        <ul>
            <li>
                Large Language Models can be used for various tasks such as text generation, translation,
                Question and answer systems, analysis and more can be used.</li>
        </ul>
        </p>
        <p>
            <li>Contextual understanding: Learning from long-term dependencies</li>
        <ul>
            <li>
                Through the self-attention mechanism, these models can also detect long-term dependencies
                Understand context, which helps them better respond to complex requests.
            </li>
        </ul>
        </p>
    </ol>
    </p>



</body>

</html>