<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Funktionsweise</title>
    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="Style.css">
</head>

<body>
    <nav>
        <a href="Geschichte.html">←</a>
        <a href="index.html">Start</a>
        <a href="Arten.html">→ </a>
    </nav>
    <h1>Funktionsweisen</h1>
    <h2>Wie funktionieren Large Language Models?</h2>
    <p>
    <ol>
        <p>
            <li>Architektur: Transformer-Netzwerke</li>
        <ul>
            <li> Die meisten Large Language Models verwenden die Transformer-Architektur. Transformer ist eine Art
                neuronales Netzwerk, das sich gut für die Verarbeitung von Sequenzen eignet, wie es bei Sprache der Fall
                ist.</li>
        </ul>
        </p>
        <p>
            <li>Aufbau von Embeddings: Kodierung von Wörtern und Kontext</li>
        <ul>
            <li>Wörter werden in Vektoren, sogenannte Embeddings, umgewandelt. Diese Vektoren erfassen die semantische
                Bedeutung von Wörtern und ihren Kontext.</li>
        </ul>
        </p>
        <p>
            <li>Selbst-Aufmerksamkeitsmechanismus: Verarbeitung von Kontext</li>
        <ul>
            <li>Der Kern des Transformer-Modells ist der Selbst-Aufmerksamkeitsmechanismus. Dieser ermöglicht es dem
                Modell, sich auf verschiedene Teile des Eingabesequenz zu konzentrieren und den Kontext besser zu
                verstehen.</li>
        </ul>
        </p>
        <p>
            <li>Vorabtraining: Pre-training auf großen Datensätzen </li>
        <ul>
            <li>Das Modell wird zunächst auf einem riesigen Datensatz vorab trainiert, der aus Texten besteht. Dabei
                lernt es, Muster und Strukturen in der Sprache zu verstehen, ohne spezifisch auf eine bestimmte Aufgabe
                trainiert zu werden.</li>
        </ul>
        </p>
        <p>
            <li>Feinabstimmung: Anpassung an spezifische Aufgaben</li>
        <ul>
            <li> Nach dem Vorabtraining wird das Modell für spezifischere Aufgaben feinabgestimmt. Dies kann die
                Anpassung an bestimmte Textarten, Übersetzungen oder andere NLP-Aufgaben umfassen.
            </li>
        </ul>
        </p>
        <p>
            <li>Anwendungsbereiche: Vielfältige NLP-Aufgaben</li>
        <ul>
            <li>Large Language Models können für verschiedene NLP-Aufgaben wie Textgenerierung, Übersetzung,
                Frage-Antwort-Systeme, Sentimentanalyse und mehr eingesetzt werden.</li>
        </ul>
        </p>
        <p>
            <li>Kontextverständnis: Lernen von langfristigen Abhängigkeiten</li>
        <ul>
            <li>Durch den Selbst-Aufmerksamkeitsmechanismus können diese Modelle auch langfristige Abhängigkeiten im
                Kontext verstehen, was dazu beiträgt, dass sie besser auf komplexe Anfragen reagieren können.
            </li>
        </ul>
        </p>
    </ol>
    </p>



</body>

</html>