<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Geschichte</title>
    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="Style.css">
</head>

<body>
    <nav>
        <a href="AllgemeineInfo.html">← </a>
        <a href="index.html">Start</a>
        <a href="Funktionsweise.html">→ </a>
    </nav>
    <h1>Geschichte</h1>
    <h2>Wie sind LLms entstanden?</h2>
    <p>
    <ul>
        <p>
            <li> Statistische Modelle und Sprachmodelle (1970er - 1990er):</li> In den 1970er und 1980er Jahren wurden
            statistische Modelle für die Sprachverarbeitung eingeführt, wodurch die Verarbeitung natürlicher Sprache
            (Natural Language Processing, NLP) verbessert wurde. Hidden Markov Models und n-gram-Modelle waren einige
            der
            frühen Ansätze. Dennoch waren diese Modelle oft auf spezifische Anwendungsfälle beschränkt.
        </p>
        <p>
            <li>Aufstieg neuronaler Netzwerke (2000er):</li> Mit dem Aufkommen von leistungsstärkeren Computern und
            größeren
            Datensätzen erlebten neuronale Netzwerke, insbesondere Recurrent Neural Networks (RNNs) und Long Short-Term
            Memory (LSTM) Netzwerke, einen Aufschwung. Diese Modelle ermöglichten eine bessere Modellierung von
            Sequenzen und zeigten verbesserte Fähigkeiten in der Sprachverarbeitung.
            <img id="img6" src="C:\Users\Admin\Desktop\llms\pktw.jpg" alt="Uhr">
        </p>
        <p>
            <li>Transformer-Architektur (2017):</li> Ein entscheidender Wendepunkt war die Einführung der
            Transformer-Architektur
            im Paper "Attention is All You Need" von Vaswani et al. im Jahr 2017. Transformer revolutionierte die Art
            und Weise, wie Modelle Sequenzen verarbeiten, indem es auf Aufmerksamkeitsmechanismen setzte. Diese
            Architektur ermöglichte eine effizientere Verarbeitung von Informationen in großen Datensätzen.
        </p>
        <li>GPT-3 und Superlative (2020):</li> Im Jahr 2020 veröffentlichte OpenAI GPT-3, ein riesiges LLM mit 175
        Milliarden
        Parametern. GPT-3 setzte neue Maßstäbe in Bezug auf Sprachverständnis, -generierung und -verarbeitung. Es
        konnte komplexe Aufgaben lösen und menschenähnlichen Text erzeugen.
    </ul>
    </p>

    <p>
    <h2 id="Timeline">Zeitleiste</h2>
    <img id="img5" src="timeline1.jpg" alt="Timeline">
    <p class="container">

        The evolution of Large Language Models (LLMs) began with ELIZA, an early chatbot in the 1960s. In the 2010s,
        Transformer architectures like BERT marked a milestone in natural language processing. GPT-3 by OpenAI (2019),
        with
        trillions of parameters, is the latest example of LLMs playing a crucial role in various applications.
    </p>
    </p>
</body>

</html>